{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/inf2.15.1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "import os \n",
    "import torch_neuronx\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NEURON_RT_NUM_CORES'] = '2'\n",
    "model_dir = \"../2.15.1/model_store/llama-2-7b-chat/llama-2-7b-chat-split\"\n",
    "model_compiled_dir=\"../2.15.1/model_store/llama-2-7b-chat/neuronx_artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model successfully loaded in 110.05396628379822 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = LlamaForSampling.from_pretrained(\n",
    "        model_dir,\n",
    "        batch_size=1,\n",
    "        tp_degree=int(os.environ['NEURON_RT_NUM_CORES']),\n",
    "        amp='f16',\n",
    "        )\n",
    "model.load(model_compiled_dir)\n",
    "model.to_neuron()\n",
    "elapsed = time.time() - start\n",
    "print(f'\\n Model successfully loaded in {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.add_special_tokens(\n",
    "        {\n",
    "        \n",
    "            \"pad_token\": \"<PAD>\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of Generative AI to me as if I am an 8 year old child. \\n Response:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "payload = {\n",
    "   \"inputs\": \"Explain the concept of Generative AI to me as if I am an 8 year old child. \\n Response:\",\n",
    "    \"parameters\": {'max_new_tokens': 50, 'top_p': 0.9, 'temperature': 0.6}\n",
    "}\n",
    "\n",
    "# run inference with top-k sampling\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = model.sample(input_ids, \n",
    "                                        sequence_length=128, \n",
    "                                        top_k=50, \n",
    "                                        temperature=0.5)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "generated_sequences = [tokenizer.decode(seq) for seq in generated_sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> Explain the concept of Generative AI to me as if I am an 8 year old child. \\n Response:\\n Generative AI is like a magic machine that can make new and exciting things! Imagine you have a toy box full of blocks, and you can use those blocks to build anything you want - a castle, a car, a robot! But with Generative AI, the machine can build things that you can't even imagine! It can make new and exciting things that you've never seen before, like a dragon or a spaceship.\\n\\nThe machine\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397\n",
      "\n",
      " Generative AI is like a magic machine that can make new and exciting things! Imagine you have a toy box full of blocks, and you can use those blocks to build anything you want - a castle, a car, a robot! But with Generative AI, the machine can build things that you can't even imagine! It can make new and exciting things that you've never seen before, like a dragon or a spaceship.\n",
      "\n",
      "The machine\n"
     ]
    }
   ],
   "source": [
    "response = generated_sequences[0].strip('<s> ')[len(prompt):]\n",
    "print(len(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 9.626596212387085 seconds\n",
      "[\n",
      "  {\n",
      "    \"generation\": \"\\n Generative AI is like a magic machine that can make new things that never existed before! It's like a super cool toy that can create anything you can imagine.\\n\\nImagine you have a big box of LEGOs, and you want to build a new toy car. You can use the LEGOs you have to build the car, but the machine that makes the car can also make new shapes and designs that you never thought of before! It's like the machine has its own imagination and can create new things that are even cooler than what you can make.\\n\\nThat's what generative AI is like! It's a machine that can make new things, like images, music, or even whole new games, just\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "payload = {\n",
    "   \"inputs\": \"Explain the concept of Generative AI to me as if I am an 8 year old child. \\n Response:\",\n",
    "    \"parameters\": {'max_new_tokens': 100, 'top_p': 0.9, 'temperature': 0.6}\n",
    "}\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "# run inference with top-k sampling\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = model.sample(input_ids, \n",
    "                                        sequence_length=payload['parameters']['max_new_tokens']+len(payload['inputs']), \n",
    "                                        top_p=payload['parameters']['top_p'], \n",
    "                                        temperature=payload['parameters']['temperature'])\n",
    "    elapsed = time.time() - start\n",
    "print(f'Inference took {elapsed} seconds')\n",
    "generated_sequences = [tokenizer.decode(seq) for seq in generated_sequences]\n",
    "response = [\n",
    "    {\n",
    "        \"generation\":generated_sequences[0].strip('<s> ')[len(prompt):]\n",
    "    }\n",
    "]\n",
    "print(json.dumps(response, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf2.15.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
